==========================================================================================
Layer (type:depth-idx)                   Output Shape              Param #
==========================================================================================
TransformerModel                         [128, 1]                  --
├─Linear: 1-1                            [128, 13, 128]            256
├─ModuleList: 1-2                        --                        --
│    └─TransformerBlock: 2-1             [128, 13, 128]            --
│    │    └─MultiHeadAttention: 3-1      [128, 13, 128]            66,304
│    │    └─FFN: 3-2                     [128, 13, 128]            526,720
│    └─TransformerBlock: 2-2             [128, 13, 128]            --
│    │    └─MultiHeadAttention: 3-3      [128, 13, 128]            66,304
│    │    └─FFN: 3-4                     [128, 13, 128]            526,720
│    └─TransformerBlock: 2-3             [128, 13, 128]            --
│    │    └─MultiHeadAttention: 3-5      [128, 13, 128]            66,304
│    │    └─FFN: 3-6                     [128, 13, 128]            526,720
│    └─TransformerBlock: 2-4             [128, 13, 128]            --
│    │    └─MultiHeadAttention: 3-7      [128, 13, 128]            66,304
│    │    └─FFN: 3-8                     [128, 13, 128]            526,720
├─Linear: 1-3                            [128, 1]                  1,665
==========================================================================================
Total params: 2,374,017
Trainable params: 2,374,017
Non-trainable params: 0
Total mult-adds (Units.MEGABYTES): 303.87
==========================================================================================
Input size (MB): 0.01
Forward/backward pass size (MB): 158.47
Params size (MB): 9.50
Estimated Total Size (MB): 167.97
==========================================================================================